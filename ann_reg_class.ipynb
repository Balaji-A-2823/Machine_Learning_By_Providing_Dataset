{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnx1b450Q/Rutxv4wvCmdx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Balaji-A-2823/Machine_Learning_By_Providing_Dataset/blob/main/ann_reg_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3MF-JwbI-EI",
        "outputId": "1c5defd6-3339-4e09-ee37-87c306cfde9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ANN Power Generation Analysis...\n",
            "============================================================\n",
            "ANN POWER GENERATION ANALYSIS - QUICK START\n",
            "============================================================\n",
            "1. Loading and preprocessing data...\n",
            "   Loaded dataset from: /content/ann_project/MW_Daily.csv\n",
            "   Dataset shape: (2742, 10)\n",
            "   Date range: 01-01-2014 to 04-07-2021\n",
            "   Created 14 features\n",
            "   Demand distribution: {'Medium': 932, 'Low': 905, 'High': 905}\n",
            "2. Preparing features for Neural Networks...\n",
            "   Using 9 features: ['n_mw', 'w_mw', 's_mw', 'e_mw', 'ne_mw', 'year', 'month_num', 'weekday', 'quarter']\n",
            "3. Training ANN Regression model...\n",
            "   ANN Regression Results:\n",
            "   - Architecture: (64, 32)\n",
            "   - Training iterations: 2000\n",
            "   - R² Score: 1.0000 (100.0% of variance explained)\n",
            "   - RMSE: 16 MW\n",
            "   - MAE: 10 MW\n",
            "4. Training ANN Classification model...\n",
            "   ANN Classification Results:\n",
            "   - Architecture: (64, 32)\n",
            "   - Training iterations: 193\n",
            "   - Accuracy: 0.9854 (98.5%)\n",
            "   - Classes: ['High', 'Low', 'Medium']\n",
            "5. Making predictions on sample data...\n",
            "   Sample Predictions:\n",
            "   Sample 1:\n",
            "     Regional Power: 150,500 MW\n",
            "     Predicted Total: 150,506 MW\n",
            "     Predicted Level: Medium\n",
            "\n",
            "   Sample 2:\n",
            "     Regional Power: 167,800 MW\n",
            "     Predicted Total: 167,796 MW\n",
            "     Predicted Level: High\n",
            "\n",
            "   Sample 3:\n",
            "     Regional Power: 136,200 MW\n",
            "     Predicted Total: 136,208 MW\n",
            "     Predicted Level: Low\n",
            "\n",
            "6. Summary:\n",
            "   ✓ ANN Regression model successfully predicts total power generation\n",
            "   ✓ ANN Classification model accurately categorizes demand levels\n",
            "   ✓ Neural networks show good performance with proper tuning\n",
            "\n",
            "============================================================\n",
            "ANN ANALYSIS COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "\n",
            "New Scenario Predictions:\n",
            "Scenario 1: Total Power = 162,612 MW, Level = High\n",
            "Scenario 2: Total Power = 184,919 MW, Level = High\n",
            "Scenario 3: Total Power = 122,006 MW, Level = Low\n",
            "\n",
            "============================================================\n",
            "KEY DIFFERENCES FROM SVM TO ANN\n",
            "============================================================\n",
            "\n",
            "1. Model Architecture:\n",
            "   - SVM: Single layer with kernel tricks\n",
            "   - ANN: Multi-layer perceptron with hidden layers\n",
            "\n",
            "2. Hyperparameters:\n",
            "   - SVM: C (regularization), kernel, gamma\n",
            "   - ANN: hidden_layer_sizes, activation, alpha, solver\n",
            "\n",
            "3. Training Process:\n",
            "   - SVM: Optimization problem solving\n",
            "   - ANN: Iterative gradient descent\n",
            "\n",
            "4. Scalability:\n",
            "   - SVM: Better for smaller datasets\n",
            "   - ANN: Better for larger datasets, more flexible\n",
            "\n",
            "5. Feature Scaling:\n",
            "   - Both require feature scaling, but ANN is more sensitive\n",
            "\n",
            "============================================================\n",
            "ANN BEST PRACTICES FOR POWER GENERATION ANALYSIS\n",
            "============================================================\n",
            "\n",
            "1. Network Architecture:\n",
            "   - Start with 1-2 hidden layers\n",
            "   - Use 64-128 neurons per layer for power data\n",
            "   - Avoid very deep networks for tabular data\n",
            "\n",
            "2. Activation Functions:\n",
            "   - ReLU: Good default choice\n",
            "   - Tanh: For data with negative values\n",
            "   - Sigmoid: For binary classification only\n",
            "\n",
            "3. Solvers:\n",
            "   - LBFGS: Best for small datasets (<1000 samples)\n",
            "   - Adam: Good for larger datasets\n",
            "   - SGD: Use with learning rate scheduling\n",
            "\n",
            "4. Regularization:\n",
            "   - Alpha: 0.001-0.1 (start with 0.01)\n",
            "   - Early stopping: Prevent overfitting\n",
            "   - Dropout: Can be added with custom implementation\n",
            "\n",
            "5. Data Preprocessing:\n",
            "   - Feature scaling is mandatory\n",
            "   - Handle missing values carefully\n",
            "   - Consider feature engineering\n"
          ]
        }
      ],
      "source": [
        "# Neural Network Power Generation Analysis - Quick Start Guide\n",
        "# This script demonstrates the ANN implementation converted from SVM\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def quick_ann_analysis(csv_file='/content/ann_project/MW_Daily.csv'):\n",
        "    \"\"\"\n",
        "    Quick ANN analysis for power generation data\n",
        "\n",
        "    This function demonstrates:\n",
        "    1. Data loading and preprocessing\n",
        "    2. ANN regression for total power prediction\n",
        "    3. ANN classification for demand level prediction\n",
        "    4. Model evaluation and predictions\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"ANN POWER GENERATION ANALYSIS - QUICK START\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Load and preprocess data\n",
        "    print(\"1. Loading and preprocessing data...\")\n",
        "\n",
        "    try:\n",
        "        # Try to load the actual CSV file\n",
        "        df = pd.read_csv(csv_file)\n",
        "        df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')\n",
        "        print(f\"   Loaded dataset from: {csv_file}\")\n",
        "    except FileNotFoundError:\n",
        "        # Create sample dataset if file not found\n",
        "        print(\"   CSV file not found. Creating sample data for demonstration...\")\n",
        "        np.random.seed(42)\n",
        "        n_samples = 1000\n",
        "\n",
        "        dates = pd.date_range(start='2020-01-01', periods=n_samples, freq='D')\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'date': dates,\n",
        "            'n_mw': np.random.normal(50000, 8000, n_samples),\n",
        "            'w_mw': np.random.normal(47000, 7000, n_samples),\n",
        "            's_mw': np.random.normal(40000, 6000, n_samples),\n",
        "            'e_mw': np.random.normal(20000, 4000, n_samples),\n",
        "            'ne_mw': np.random.normal(2500, 500, n_samples),\n",
        "            'year': dates.year,\n",
        "        })\n",
        "\n",
        "        # Create ind_mw as sum with some noise\n",
        "        df['ind_mw'] = (df['n_mw'] + df['w_mw'] + df['s_mw'] +\n",
        "                        df['e_mw'] + df['ne_mw'] +\n",
        "                        np.random.normal(0, 5000, n_samples))\n",
        "\n",
        "    print(f\"   Dataset shape: {df.shape}\")\n",
        "    print(f\"   Date range: {df['date'].iloc[0].strftime('%d-%m-%Y')} to {df['date'].iloc[-1].strftime('%d-%m-%Y')}\")\n",
        "\n",
        "    # Create additional features\n",
        "    df['month_num'] = df['date'].dt.month\n",
        "    df['weekday'] = df['date'].dt.weekday\n",
        "    df['quarter'] = df['date'].dt.quarter\n",
        "\n",
        "    # Create demand categories\n",
        "    q1, q2 = df['ind_mw'].quantile([0.33, 0.67])\n",
        "    conditions = [\n",
        "        df['ind_mw'] <= q1,\n",
        "        (df['ind_mw'] > q1) & (df['ind_mw'] <= q2),\n",
        "        df['ind_mw'] > q2\n",
        "    ]\n",
        "    choices = ['Low', 'Medium', 'High']\n",
        "    df['demand_class'] = np.select(conditions, choices, default='Unknown')\n",
        "\n",
        "    print(f\"   Created {len(df.columns)} features\")\n",
        "    print(f\"   Demand distribution: {df['demand_class'].value_counts().to_dict()}\")\n",
        "\n",
        "    # Step 2: Prepare features\n",
        "    print(\"2. Preparing features for Neural Networks...\")\n",
        "\n",
        "    features = ['n_mw', 'w_mw', 's_mw', 'e_mw', 'ne_mw',\n",
        "                'year', 'month_num', 'weekday', 'quarter']\n",
        "\n",
        "    X = df[features]\n",
        "    y_regression = df['ind_mw']  # Total power (continuous)\n",
        "    y_classification = df['demand_class']  # Demand level (categorical)\n",
        "\n",
        "    print(f\"   Using {len(features)} features: {features}\")\n",
        "\n",
        "    # Step 3: Train ANN Regression Model\n",
        "    print(\"3. Training ANN Regression model...\")\n",
        "\n",
        "    # Split data for regression\n",
        "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "        X, y_regression, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Scale features (very important for neural networks)\n",
        "    scaler_reg = StandardScaler()\n",
        "    X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
        "    X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
        "\n",
        "    # Train ANN regression model\n",
        "    # MLPRegressor parameters explained:\n",
        "    # - hidden_layer_sizes: Architecture of hidden layers\n",
        "    # - activation: Activation function for hidden layers\n",
        "    # - solver: Algorithm for weight optimization\n",
        "    # - alpha: L2 penalty parameter for regularization\n",
        "    # - max_iter: Maximum iterations for convergence\n",
        "\n",
        "    ann_regressor = MLPRegressor(\n",
        "        hidden_layer_sizes=(64, 32),   # Two hidden layers with 64 and 32 neurons\n",
        "        activation='relu',             # ReLU activation function\n",
        "        solver='lbfgs',               # LBFGS optimizer (good for smaller datasets)\n",
        "        alpha=0.01,                   # L2 regularization\n",
        "        max_iter=2000,                # Maximum iterations\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    ann_regressor.fit(X_train_reg_scaled, y_train_reg)\n",
        "\n",
        "    # Evaluate regression\n",
        "    y_pred_reg = ann_regressor.predict(X_test_reg_scaled)\n",
        "    r2_reg = r2_score(y_test_reg, y_pred_reg)\n",
        "    rmse_reg = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
        "    mae_reg = np.mean(np.abs(y_test_reg - y_pred_reg))\n",
        "\n",
        "    print(f\"   ANN Regression Results:\")\n",
        "    print(f\"   - Architecture: {ann_regressor.hidden_layer_sizes}\")\n",
        "    print(f\"   - Training iterations: {ann_regressor.n_iter_}\")\n",
        "    print(f\"   - R² Score: {r2_reg:.4f} ({r2_reg*100:.1f}% of variance explained)\")\n",
        "    print(f\"   - RMSE: {rmse_reg:.0f} MW\")\n",
        "    print(f\"   - MAE: {mae_reg:.0f} MW\")\n",
        "\n",
        "    # Step 4: Train ANN Classification Model\n",
        "    print(\"4. Training ANN Classification model...\")\n",
        "\n",
        "    # Encode categorical labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_classification_encoded = label_encoder.fit_transform(y_classification)\n",
        "\n",
        "    # Split data for classification\n",
        "    X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
        "        X, y_classification_encoded, test_size=0.2, random_state=42,\n",
        "        stratify=y_classification_encoded\n",
        "    )\n",
        "\n",
        "    # Scale features for classification\n",
        "    scaler_clf = StandardScaler()\n",
        "    X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\n",
        "    X_test_clf_scaled = scaler_clf.transform(X_test_clf)\n",
        "\n",
        "    # Train ANN classification model\n",
        "    ann_classifier = MLPClassifier(\n",
        "        hidden_layer_sizes=(64, 32),   # Two hidden layers\n",
        "        activation='relu',             # ReLU activation\n",
        "        solver='adam',                 # Adam optimizer\n",
        "        alpha=0.01,                   # L2 regularization\n",
        "        max_iter=500,                 # Maximum iterations\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    ann_classifier.fit(X_train_clf_scaled, y_train_clf)\n",
        "\n",
        "    # Evaluate classification\n",
        "    y_pred_clf = ann_classifier.predict(X_test_clf_scaled)\n",
        "    accuracy_clf = accuracy_score(y_test_clf, y_pred_clf)\n",
        "\n",
        "    # Convert back to original labels for reporting\n",
        "    y_pred_clf_labels = label_encoder.inverse_transform(y_pred_clf)\n",
        "    y_test_clf_labels = label_encoder.inverse_transform(y_test_clf)\n",
        "\n",
        "    print(f\"   ANN Classification Results:\")\n",
        "    print(f\"   - Architecture: {ann_classifier.hidden_layer_sizes}\")\n",
        "    print(f\"   - Training iterations: {ann_classifier.n_iter_}\")\n",
        "    print(f\"   - Accuracy: {accuracy_clf:.4f} ({accuracy_clf*100:.1f}%)\")\n",
        "    print(f\"   - Classes: {list(label_encoder.classes_)}\")\n",
        "\n",
        "    # Step 5: Make Predictions on Sample Data\n",
        "    print(\"5. Making predictions on sample data...\")\n",
        "\n",
        "    # Create sample data\n",
        "    sample_data = pd.DataFrame({\n",
        "        'n_mw': [45000, 55000, 40000],\n",
        "        'w_mw': [46000, 48000, 42000],\n",
        "        's_mw': [38000, 41000, 35000],\n",
        "        'e_mw': [19000, 21000, 17000],\n",
        "        'ne_mw': [2500, 2800, 2200],\n",
        "        'year': [2021, 2021, 2021],\n",
        "        'month_num': [6, 6, 12],\n",
        "        'weekday': [1, 3, 5],\n",
        "        'quarter': [2, 2, 4]\n",
        "    })\n",
        "\n",
        "    # Scale sample data\n",
        "    sample_scaled_reg = scaler_reg.transform(sample_data[features])\n",
        "    sample_scaled_clf = scaler_clf.transform(sample_data[features])\n",
        "\n",
        "    # Make predictions\n",
        "    total_power_pred = ann_regressor.predict(sample_scaled_reg)\n",
        "    demand_level_pred_enc = ann_classifier.predict(sample_scaled_clf)\n",
        "    demand_level_pred = label_encoder.inverse_transform(demand_level_pred_enc)\n",
        "\n",
        "    print(\"   Sample Predictions:\")\n",
        "    for i in range(len(sample_data)):\n",
        "        regional_sum = (sample_data.iloc[i]['n_mw'] + sample_data.iloc[i]['w_mw'] +\n",
        "                       sample_data.iloc[i]['s_mw'] + sample_data.iloc[i]['e_mw'] +\n",
        "                       sample_data.iloc[i]['ne_mw'])\n",
        "\n",
        "        print(f\"   Sample {i+1}:\")\n",
        "        print(f\"     Regional Power: {regional_sum:,} MW\")\n",
        "        print(f\"     Predicted Total: {total_power_pred[i]:,.0f} MW\")\n",
        "        print(f\"     Predicted Level: {demand_level_pred[i]}\")\n",
        "        print()\n",
        "\n",
        "    # Step 6: Summary\n",
        "    print(\"6. Summary:\")\n",
        "    print(\"   ✓ ANN Regression model successfully predicts total power generation\")\n",
        "    print(\"   ✓ ANN Classification model accurately categorizes demand levels\")\n",
        "    print(\"   ✓ Neural networks show good performance with proper tuning\")\n",
        "\n",
        "    # Return trained models for further use\n",
        "    models = {\n",
        "        'regression': {\n",
        "            'model': ann_regressor,\n",
        "            'scaler': scaler_reg,\n",
        "            'features': features,\n",
        "            'metrics': {'r2': r2_reg, 'rmse': rmse_reg, 'mae': mae_reg}\n",
        "        },\n",
        "        'classification': {\n",
        "            'model': ann_classifier,\n",
        "            'scaler': scaler_clf,\n",
        "            'label_encoder': label_encoder,\n",
        "            'features': features,\n",
        "            'metrics': {'accuracy': accuracy_clf}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANN ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def predict_new_data_ann(models, new_data):\n",
        "    \"\"\"\n",
        "    Use trained ANN models to predict on new data\n",
        "\n",
        "    Parameters:\n",
        "    - models: Dictionary with trained models from quick_ann_analysis()\n",
        "    - new_data: DataFrame with required feature columns\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with predictions\n",
        "    \"\"\"\n",
        "\n",
        "    features = models['regression']['features']\n",
        "\n",
        "    # Regression prediction\n",
        "    reg_scaler = models['regression']['scaler']\n",
        "    reg_model = models['regression']['model']\n",
        "    X_reg_scaled = reg_scaler.transform(new_data[features])\n",
        "    total_power_pred = reg_model.predict(X_reg_scaled)\n",
        "\n",
        "    # Classification prediction\n",
        "    clf_scaler = models['classification']['scaler']\n",
        "    clf_model = models['classification']['model']\n",
        "    label_encoder = models['classification']['label_encoder']\n",
        "    X_clf_scaled = clf_scaler.transform(new_data[features])\n",
        "    demand_level_pred_enc = clf_model.predict(X_clf_scaled)\n",
        "    demand_level_pred = label_encoder.inverse_transform(demand_level_pred_enc)\n",
        "\n",
        "    # Combine results\n",
        "    predictions = pd.DataFrame({\n",
        "        'predicted_total_power': total_power_pred,\n",
        "        'predicted_demand_level': demand_level_pred\n",
        "    })\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def compare_ann_architectures(X, y_reg, y_clf, features):\n",
        "    \"\"\"\n",
        "    Compare different ANN architectures and hyperparameters\n",
        "\n",
        "    Parameters:\n",
        "    - X, y_reg, y_clf: Training data\n",
        "    - features: Feature names\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary with comparison results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COMPARING DIFFERENT ANN ARCHITECTURES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Different architectures to test\n",
        "    architectures = [\n",
        "        (32,),           # Single hidden layer\n",
        "        (64, 32),        # Two hidden layers\n",
        "        (100, 50, 25),   # Three hidden layers\n",
        "        (128, 64, 32)    # Deeper network\n",
        "    ]\n",
        "\n",
        "    # Different solvers to test\n",
        "    solvers = ['lbfgs', 'adam', 'sgd']\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Encode classification labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_clf_encoded = label_encoder.fit_transform(y_clf)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_reg_train, y_reg_test = train_test_split(\n",
        "        X, y_reg, test_size=0.2, random_state=42\n",
        "    )\n",
        "    _, _, y_clf_train, y_clf_test = train_test_split(\n",
        "        X, y_clf_encoded, test_size=0.2, random_state=42, stratify=y_clf_encoded\n",
        "    )\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    for arch in architectures:\n",
        "        for solver in solvers:\n",
        "            try:\n",
        "                # Test regression\n",
        "                reg_model = MLPRegressor(\n",
        "                    hidden_layer_sizes=arch,\n",
        "                    solver=solver,\n",
        "                    alpha=0.01,\n",
        "                    max_iter=500,\n",
        "                    random_state=42\n",
        "                )\n",
        "                reg_model.fit(X_train_scaled, y_reg_train)\n",
        "                reg_pred = reg_model.predict(X_test_scaled)\n",
        "                reg_r2 = r2_score(y_reg_test, reg_pred)\n",
        "\n",
        "                # Test classification\n",
        "                clf_model = MLPClassifier(\n",
        "                    hidden_layer_sizes=arch,\n",
        "                    solver=solver,\n",
        "                    alpha=0.01,\n",
        "                    max_iter=500,\n",
        "                    random_state=42\n",
        "                )\n",
        "                clf_model.fit(X_train_scaled, y_clf_train)\n",
        "                clf_pred = clf_model.predict(X_test_scaled)\n",
        "                clf_accuracy = accuracy_score(y_clf_test, clf_pred)\n",
        "\n",
        "                results.append({\n",
        "                    'architecture': arch,\n",
        "                    'solver': solver,\n",
        "                    'regression_r2': reg_r2,\n",
        "                    'classification_accuracy': clf_accuracy,\n",
        "                    'reg_iterations': reg_model.n_iter_,\n",
        "                    'clf_iterations': clf_model.n_iter_\n",
        "                })\n",
        "\n",
        "                print(f\"   {arch} + {solver}: R²={reg_r2:.3f}, Accuracy={clf_accuracy:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   {arch} + {solver}: Failed ({str(e)[:50]}...)\")\n",
        "\n",
        "    # Find best configurations\n",
        "    best_reg = max(results, key=lambda x: x['regression_r2'])\n",
        "    best_clf = max(results, key=lambda x: x['classification_accuracy'])\n",
        "\n",
        "    print(f\"\\nBest Regression: {best_reg['architecture']} + {best_reg['solver']} (R²={best_reg['regression_r2']:.3f})\")\n",
        "    print(f\"Best Classification: {best_clf['architecture']} + {best_clf['solver']} (Acc={best_clf['classification_accuracy']:.3f})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Run the quick analysis\n",
        "        print(\"Starting ANN Power Generation Analysis...\")\n",
        "        trained_models = quick_ann_analysis(\"/content/ann_project/MW_Daily.csv\")\n",
        "\n",
        "        if trained_models:\n",
        "            # Test with new scenarios\n",
        "            new_scenarios = pd.DataFrame({\n",
        "                'n_mw': [50000, 60000, 35000],\n",
        "                'w_mw': [48000, 52000, 38000],\n",
        "                's_mw': [42000, 46000, 32000],\n",
        "                'e_mw': [20000, 24000, 15000],\n",
        "                'ne_mw': [2600, 2900, 2000],\n",
        "                'year': [2024, 2024, 2024],\n",
        "                'month_num': [7, 8, 3],\n",
        "                'weekday': [0, 2, 4],\n",
        "                'quarter': [3, 3, 1]\n",
        "            })\n",
        "\n",
        "            predictions = predict_new_data_ann(trained_models, new_scenarios)\n",
        "\n",
        "            print(\"\\nNew Scenario Predictions:\")\n",
        "            for i in range(len(new_scenarios)):\n",
        "                print(f\"Scenario {i+1}: Total Power = {predictions.iloc[i]['predicted_total_power']:,.0f} MW, \"\n",
        "                      f\"Level = {predictions.iloc[i]['predicted_demand_level']}\")\n",
        "\n",
        "        # Display key differences from SVM\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"KEY DIFFERENCES FROM SVM TO ANN\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(\"\\n1. Model Architecture:\")\n",
        "        print(\"   - SVM: Single layer with kernel tricks\")\n",
        "        print(\"   - ANN: Multi-layer perceptron with hidden layers\")\n",
        "\n",
        "        print(\"\\n2. Hyperparameters:\")\n",
        "        print(\"   - SVM: C (regularization), kernel, gamma\")\n",
        "        print(\"   - ANN: hidden_layer_sizes, activation, alpha, solver\")\n",
        "\n",
        "        print(\"\\n3. Training Process:\")\n",
        "        print(\"   - SVM: Optimization problem solving\")\n",
        "        print(\"   - ANN: Iterative gradient descent\")\n",
        "\n",
        "        print(\"\\n4. Scalability:\")\n",
        "        print(\"   - SVM: Better for smaller datasets\")\n",
        "        print(\"   - ANN: Better for larger datasets, more flexible\")\n",
        "\n",
        "        print(\"\\n5. Feature Scaling:\")\n",
        "        print(\"   - Both require feature scaling, but ANN is more sensitive\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        print(\"Please ensure the data file exists or the script will use sample data.\")\n",
        "\n",
        "# Additional ANN-specific tips and best practices\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANN BEST PRACTICES FOR POWER GENERATION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n1. Network Architecture:\")\n",
        "print(\"   - Start with 1-2 hidden layers\")\n",
        "print(\"   - Use 64-128 neurons per layer for power data\")\n",
        "print(\"   - Avoid very deep networks for tabular data\")\n",
        "\n",
        "print(\"\\n2. Activation Functions:\")\n",
        "print(\"   - ReLU: Good default choice\")\n",
        "print(\"   - Tanh: For data with negative values\")\n",
        "print(\"   - Sigmoid: For binary classification only\")\n",
        "\n",
        "print(\"\\n3. Solvers:\")\n",
        "print(\"   - LBFGS: Best for small datasets (<1000 samples)\")\n",
        "print(\"   - Adam: Good for larger datasets\")\n",
        "print(\"   - SGD: Use with learning rate scheduling\")\n",
        "\n",
        "print(\"\\n4. Regularization:\")\n",
        "print(\"   - Alpha: 0.001-0.1 (start with 0.01)\")\n",
        "print(\"   - Early stopping: Prevent overfitting\")\n",
        "print(\"   - Dropout: Can be added with custom implementation\")\n",
        "\n",
        "print(\"\\n5. Data Preprocessing:\")\n",
        "print(\"   - Feature scaling is mandatory\")\n",
        "print(\"   - Handle missing values carefully\")\n",
        "print(\"   - Consider feature engineering\")"
      ]
    }
  ]
}